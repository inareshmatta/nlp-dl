{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c850c7-dc50-4ca3-a01f-9a4fed3d32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "def create_complete_neural_network_diagram():\n",
    "    # Create a new directed graph for the neural network\n",
    "    dot = graphviz.Digraph('Neural_Network', comment='Neural Network Architecture', format='svg')\n",
    "    \n",
    "    # Set graph attributes\n",
    "    dot.attr(rankdir='LR')  # Left to Right layout\n",
    "    dot.attr(nodesep='1.0')  # Increase space between nodes\n",
    "    dot.attr(ranksep='2.5')  # Increase space between layers\n",
    "    \n",
    "    # Input layer\n",
    "    with dot.subgraph(name='cluster_input') as c:\n",
    "        c.attr(style='filled', color='lightblue', margin='40')\n",
    "        c.node_attr.update(style='filled', color='white', shape='circle', fixedsize='true', width='0.8')\n",
    "        \n",
    "        # Show all 10 input neurons\n",
    "        for i in range(10):\n",
    "            c.node(f'i{i}', f'')\n",
    "        \n",
    "        c.attr(label='Input Layer\\n(Word Embedding)\\nDimension: 10')\n",
    "    \n",
    "    # Hidden layer\n",
    "    with dot.subgraph(name='cluster_hidden') as c:\n",
    "        c.attr(style='filled', color='lightgreen', margin='40')\n",
    "        c.node_attr.update(style='filled', color='white', shape='circle', fixedsize='true', width='0.8')\n",
    "        \n",
    "        for i in range(5):  # 5 hidden neurons\n",
    "            c.node(f'h{i}', f'')\n",
    "        \n",
    "        c.attr(label='Hidden Layer\\n(ReLU)\\nNeurons: 5')\n",
    "    \n",
    "    # Output layer - explicitly adding all output neurons\n",
    "    with dot.subgraph(name='cluster_output') as c:\n",
    "        c.attr(style='filled', color='lightyellow', margin='40')\n",
    "        c.node_attr.update(style='filled', color='white', shape='circle', fixedsize='true', width='0.8')\n",
    "        \n",
    "        # Explicitly create all output neurons\n",
    "        c.node('o0', '')\n",
    "        c.node('o1', '')\n",
    "        c.node('o2', '')\n",
    "        \n",
    "        c.attr(label='Output Layer\\n(Softmax)\\nNeurons: 3')\n",
    "    \n",
    "    # Add class labels\n",
    "    dot.node('l0', 'Positive', shape='box', style='filled', fillcolor='lightpink')\n",
    "    dot.node('l1', 'Neutral', shape='box', style='filled', fillcolor='lightpink')\n",
    "    dot.node('l2', 'Negative', shape='box', style='filled', fillcolor='lightpink')\n",
    "    \n",
    "    # Connect output neurons to class labels\n",
    "    dot.edge('o0', 'l0', style='dashed')\n",
    "    dot.edge('o1', 'l1', style='dashed')\n",
    "    dot.edge('o2', 'l2', style='dashed')\n",
    "    \n",
    "    # Connect a subset of neurons between layers with weight values\n",
    "    # Input to hidden (only show some connections for clarity)\n",
    "    weights = {\n",
    "        'i0_h0': 0.7, 'i1_h0': -0.3, 'i2_h1': 0.5, 'i3_h1': -0.6,\n",
    "        'i4_h2': 0.8, 'i5_h2': -0.4, 'i6_h3': 0.3, 'i7_h3': -0.5,\n",
    "        'i8_h4': 0.6, 'i9_h4': -0.2,\n",
    "        'h0_o0': 0.8, 'h0_o1': -0.3, 'h0_o2': -0.5,\n",
    "        'h1_o0': -0.4, 'h1_o1': 0.7, 'h1_o2': -0.3,\n",
    "        'h2_o0': 0.3, 'h2_o1': -0.2, 'h2_o2': -0.5,\n",
    "        'h3_o0': -0.6, 'h3_o1': 0.4, 'h3_o2': 0.2,\n",
    "        'h4_o0': 0.5, 'h4_o1': -0.3, 'h4_o2': 0.8\n",
    "    }\n",
    "    \n",
    "    # Connect input to hidden\n",
    "    connections = [\n",
    "        ('0', '0'), ('1', '0'), ('2', '1'), ('3', '1'),\n",
    "        ('4', '2'), ('5', '2'), ('6', '3'), ('7', '3'), \n",
    "        ('8', '4'), ('9', '4')\n",
    "    ]\n",
    "    \n",
    "    for i, h in connections:\n",
    "        key = f'i{i}_h{h}'\n",
    "        weight = weights[key]\n",
    "        color = \"red\" if weight < 0 else \"green\"\n",
    "        penwidth = str(abs(weight) * 2.5)\n",
    "        dot.edge(f'i{i}', f'h{h}', label=f'{weight:.1f}', color=color, penwidth=penwidth)\n",
    "    \n",
    "    # Connect hidden to output - show ALL connections for the output layer\n",
    "    for h in range(5):\n",
    "        for o in range(3):\n",
    "            key = f'h{h}_o{o}'\n",
    "            weight = weights[key]\n",
    "            color = \"red\" if weight < 0 else \"green\"\n",
    "            penwidth = str(abs(weight) * 2.5)\n",
    "            dot.edge(f'h{h}', f'o{o}', label=f'{weight:.1f}', color=color, penwidth=penwidth)\n",
    "    \n",
    "    # Add biases\n",
    "    dot.node('b_h', 'Hidden Layer Biases', shape='box', style='filled', fillcolor='lightcyan')\n",
    "    for i in range(5):\n",
    "        bias_value = round(0.1 * (i+1), 1)\n",
    "        dot.edge('b_h', f'h{i}', style='dotted', label=f'+{bias_value}')\n",
    "    \n",
    "    dot.node('b_o', 'Output Layer Biases', shape='box', style='filled', fillcolor='lightcyan')\n",
    "    for i in range(3):\n",
    "        bias_value = round(0.2 * (i+1), 1)\n",
    "        dot.edge('b_o', f'o{i}', style='dotted', label=f'+{bias_value}')\n",
    "    \n",
    "    # Activation functions\n",
    "    dot.node('relu', 'ReLU Activation:\\nf(x) = max(0,x)', shape='note', style='filled', fillcolor='lightgrey')\n",
    "    dot.edge('h0', 'relu', style='dashed', dir='none')\n",
    "    \n",
    "    dot.node('softmax', 'Softmax Activation:\\nf(x_i) = e^x_i / Σ e^x_j', shape='note', style='filled', fillcolor='lightgrey')\n",
    "    dot.edge('o0', 'softmax', style='dashed', dir='none')\n",
    "    \n",
    "    # Forward pass explanation\n",
    "    dot.node('fwd_pass', \"\"\"Forward Pass Steps:\n",
    "1. Input × Weights_1 + Bias_1 = Hidden Layer Input\n",
    "2. ReLU(Hidden Layer Input) = Hidden Layer Output\n",
    "3. Hidden Output × Weights_2 + Bias_2 = Output Layer Input\n",
    "4. Softmax(Output Layer Input) = Final Probabilities\"\"\", shape='box', style='filled', fillcolor='lightsalmon')\n",
    "    \n",
    "    # Backpropagation explanation\n",
    "    dot.node('backprop', \"\"\"Backpropagation Steps:\n",
    "1. Calculate error at output layer\n",
    "2. Compute gradients of weights and biases\n",
    "3. Propagate error backward to hidden layer\n",
    "4. Update all weights and biases using gradient descent\n",
    "   w_new = w_old - learning_rate * gradient\"\"\", shape='box', style='filled', fillcolor='lightsalmon')\n",
    "    \n",
    "    # Legend\n",
    "    dot.node('legend', \"\"\"Weight Legend:\n",
    "Green line = Positive weight\n",
    "Red line = Negative weight\n",
    "Line thickness = Weight magnitude\n",
    "Dotted line = Bias connection\n",
    "\n",
    "The neural network learns by adjusting\n",
    "weights and biases during training.\"\"\", shape='box', style='filled', fillcolor='lightgrey')\n",
    "    \n",
    "    # Add example of signal propagation\n",
    "    dot.node('example', \"\"\"Example Computation (for one neuron):\n",
    "Input to h0 = (0.7 × i0) + (-0.3 × i1) + b_h0\n",
    "If input values are [0.5, 0.8]:\n",
    "  = (0.7 × 0.5) + (-0.3 × 0.8) + 0.1\n",
    "  = 0.35 - 0.24 + 0.1 = 0.21\n",
    "After ReLU: max(0, 0.21) = 0.21\"\"\", shape='box', style='filled', fillcolor='lightgrey')\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Full tokenization and embedding diagram\n",
    "def create_detailed_tokenization_and_embedding_diagram():\n",
    "    # Create a new directed graph\n",
    "    dot = graphviz.Digraph('Tokenization_and_Embedding', comment='NLP Tokenization and Embedding Process', format='svg')\n",
    "    \n",
    "    # Set graph attributes\n",
    "    dot.attr(rankdir='TB')  # Top to Bottom layout\n",
    "    dot.attr(nodesep='0.8')  # Space between nodes\n",
    "    dot.attr(ranksep='1.5')  # Space between ranks\n",
    "\n",
    "    # Original text\n",
    "    dot.node('text', '\"Natural language processing helps computers understand human language.\"', \n",
    "             shape='box', style='filled', fillcolor='lightblue', width='8')\n",
    "    \n",
    "    # Step 1: Cleaning\n",
    "    dot.node('clean_step', 'Step 1: Text Cleaning\\n(lowercase, remove punctuation, etc.)', \n",
    "            shape='box', style='filled', fillcolor='lightgreen')\n",
    "    dot.edge('text', 'clean_step')\n",
    "    \n",
    "    # Cleaned text\n",
    "    dot.node('cleaned_text', '\"natural language processing helps computers understand human language\"', \n",
    "             shape='box', style='filled', fillcolor='lightgreen', width='8')\n",
    "    dot.edge('clean_step', 'cleaned_text')\n",
    "    \n",
    "    # Step 2: Tokenization\n",
    "    dot.node('token_step', 'Step 2: Tokenization\\n(split text into individual words)', \n",
    "            shape='box', style='filled', fillcolor='lightyellow')\n",
    "    dot.edge('cleaned_text', 'token_step')\n",
    "    \n",
    "    # Create all tokens\n",
    "    tokens = ['natural', 'language', 'processing', 'helps', 'computers', 'understand', 'human', 'language']\n",
    "    \n",
    "    with dot.subgraph(name='cluster_tokens') as c:\n",
    "        c.attr(style='filled', color='lightyellow')\n",
    "        c.attr(label='Individual Tokens')\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            # Split tokens into two rows for better visibility\n",
    "            row = i // 4\n",
    "            col = i % 4\n",
    "            c.node(f't{i}', f'\"{token}\"', shape='box')\n",
    "    \n",
    "    # Connect tokenization step to all tokens\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge('token_step', f't{i}')\n",
    "    \n",
    "    # Step 3: Vocabulary Lookup\n",
    "    dot.node('vocab_step', 'Step 3: Vocabulary Mapping\\n(convert words to unique IDs)', \n",
    "            shape='box', style='filled', fillcolor='lightpink')\n",
    "    \n",
    "    # Connect tokens to vocabulary step\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge(f't{i}', 'vocab_step')\n",
    "    \n",
    "    # Token to ID mapping - show ALL tokens\n",
    "    with dot.subgraph(name='cluster_mapping') as c:\n",
    "        c.attr(style='filled', color='lightpink')\n",
    "        c.attr(label='Token to ID Mapping')\n",
    "        \n",
    "        # Create a table-like structure for token-ID pairs\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_id = 10 + i  # Example IDs\n",
    "            c.node(f'pair{i}', f'\"{token}\" → {token_id}', shape='box')\n",
    "    \n",
    "    # Connect vocabulary step to all token-ID pairs\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge('vocab_step', f'pair{i}')\n",
    "    \n",
    "    # Step 4: Embedding Lookup\n",
    "    dot.node('embed_step', 'Step 4: Embedding Lookup\\n(convert token IDs to vectors)', \n",
    "            shape='box', style='filled', fillcolor='lightsalmon')\n",
    "    \n",
    "    # Connect all token-ID pairs to embedding step\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge(f'pair{i}', 'embed_step')\n",
    "    \n",
    "    # Embedding vectors - show ALL tokens\n",
    "    with dot.subgraph(name='cluster_embeddings') as c:\n",
    "        c.attr(style='filled', color='lightsalmon')\n",
    "        c.attr(label='Word Embedding Vectors (Dimension = 5)')\n",
    "        \n",
    "        # Create embedding vector representations for all tokens\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Generate some pseudo-random values for demonstration\n",
    "            vector = [(0.2 * i) % 1, (-0.3 * i) % 1, (0.5 * i) % 1, (-0.1 * i) % 1, (0.4 * i) % 1]\n",
    "            vector_str = \", \".join([f\"{val:.1f}\" for val in vector])\n",
    "            c.node(f'emb{i}', f'\"{token}\" → [{vector_str}]', shape='box')\n",
    "    \n",
    "    # Connect embedding step to all embedding vectors\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge('embed_step', f'emb{i}')\n",
    "    \n",
    "    # Step 5: Document Vector\n",
    "    dot.node('doc_step', 'Step 5: Create Document Vector\\n(Average all word vectors)', \n",
    "            shape='box', style='filled', fillcolor='lightcyan')\n",
    "    \n",
    "    # Connect all embeddings to document vector step\n",
    "    for i in range(len(tokens)):\n",
    "        dot.edge(f'emb{i}', 'doc_step')\n",
    "    \n",
    "    # Final document vector\n",
    "    dot.node('doc_vector', 'Document Vector: [0.23, -0.15, 0.35, -0.08, 0.19]', \n",
    "            shape='box', style='filled', fillcolor='lightcyan', width='6')\n",
    "    dot.edge('doc_step', 'doc_vector')\n",
    "    \n",
    "    # Step 6: Neural Network Input\n",
    "    dot.node('nn_input', 'Step 6: Feed to Neural Network\\n(for classification or other tasks)', \n",
    "            shape='box', style='filled', fillcolor='lavender')\n",
    "    dot.edge('doc_vector', 'nn_input')\n",
    "    \n",
    "    # Add explanation of the embedding space\n",
    "    dot.node('embedding_explanation', \"\"\"Word Embedding Properties:\n",
    "1. Similar words have similar vectors\n",
    "2. Vector arithmetic captures semantic relationships\n",
    "   e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\"\n",
    "3. Distance between vectors represents semantic similarity\n",
    "4. Embeddings can be learned from data or pre-trained\"\"\", \n",
    "            shape='note', style='filled', fillcolor='lightgrey')\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Create and display the diagrams\n",
    "nn_diagram = create_complete_neural_network_diagram()\n",
    "nn_diagram.render('complete_neural_network_diagram', view=True)\n",
    "print(\"SVG diagram created as 'complete_neural_network_diagram.svg'\")\n",
    "\n",
    "tokenization_diagram = create_detailed_tokenization_and_embedding_diagram()\n",
    "tokenization_diagram.render('detailed_tokenization_diagram', view=True)\n",
    "print(\"SVG diagram created as 'detailed_tokenization_diagram.svg'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cfdd9-80c2-4108-b26f-c6c2ab772245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754edd5-583b-415d-a210-93d435355ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction and Document Loading\n",
    "\n",
    "# Install required packages\n",
    "!pip install numpy matplotlib PyPDF2 python-docx tqdm pandas seaborn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d19041-0079-4e89-a9c4-2ec76f15e681",
   "metadata": {},
   "source": [
    "# Natural Language Processing from Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements a complete NLP pipeline and deep learning model using only Python and NumPy. No high-level libraries like TensorFlow, PyTorch, spaCy, or NLTK are used for the core functionality - we build everything from the ground up.\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. **Document Loading**: Reading text from various file formats (PDF, DOCX, TXT)\n",
    "2. **Text Preprocessing**: Tokenization, cleaning, and normalization\n",
    "3. **Feature Engineering**: Converting text to numerical representations\n",
    "4. **Model Building**: Implementing neural networks from scratch\n",
    "5. **Training & Evaluation**: Learning from data and measuring performance\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "While we'll build most functionality from scratch, we'll need a few utility libraries for file handling and basic operations:\n",
    "- Python 3.6+\n",
    "- NumPy for numerical operations\n",
    "- PyPDF2 for PDF reading\n",
    "- python-docx for DOCX reading\n",
    "- Matplotlib for visualizations\n",
    "- Pandas for tabular displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476fd19-01cf-405e-bc55-754849549b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"A class to load text from various document formats.\n",
    "    \n",
    "    This class provides methods to extract text from PDFs, Word documents,\n",
    "    plain text files, and even directories containing multiple files.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text_file(file_path):\n",
    "        \"\"\"Load text from a plain text file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the text file\n",
    "            \n",
    "        Returns:\n",
    "            str: The content of the text file\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf(file_path):\n",
    "        \"\"\"Extract text from a PDF file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            str: The extracted text content\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {e}\")\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_docx(file_path):\n",
    "        \"\"\"Extract text from a Word document.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the DOCX file\n",
    "            \n",
    "        Returns:\n",
    "            str: The extracted text content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(file_path)\n",
    "            return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_file(file_path):\n",
    "        \"\"\"Load text from a file based on its extension.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the file\n",
    "            \n",
    "        Returns:\n",
    "            str: The extracted text content\n",
    "        \"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            return DocumentLoader.load_pdf(file_path)\n",
    "        elif file_extension == '.docx':\n",
    "            return DocumentLoader.load_docx(file_path)\n",
    "        elif file_extension == '.txt':\n",
    "            return DocumentLoader.load_text_file(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_extension}\")\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_directory(directory_path, extensions=None):\n",
    "        \"\"\"Load all supported documents from a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path (str): Path to the directory\n",
    "            extensions (list, optional): List of file extensions to include. \n",
    "                                        Defaults to ['.txt', '.pdf', '.docx']\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary mapping filenames to their text content\n",
    "        \"\"\"\n",
    "        if extensions is None:\n",
    "            extensions = ['.txt', '.pdf', '.docx']\n",
    "            \n",
    "        documents = {}\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    documents[file] = DocumentLoader.load_file(file_path)\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4bab9-543a-490a-9cf1-bac942127b00",
   "metadata": {},
   "source": [
    "## 1. Document Loading\n",
    "\n",
    "First, we'll create functionality to load text from different document formats. This is the starting point of any NLP pipeline.\n",
    "\n",
    "### Why Document Loading Matters\n",
    "\n",
    "In real-world applications, text data comes in various formats. Being able to handle different file types is essential for building robust NLP systems. Each file format has its own structure and requires specific parsing approaches.\n",
    "\n",
    "### Using the Document Loader\n",
    "\n",
    "The `DocumentLoader` class we just created can handle multiple document formats:\n",
    "- **PDF files**: Uses PyPDF2 to extract text from each page\n",
    "- **Word documents**: Uses python-docx to extract paragraph text\n",
    "- **Plain text files**: Simply reads the content\n",
    "\n",
    "Let's create a sample text file to test our document loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd79576-c55e-4dc9-a2c3-f1251f5c9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example text file to demonstrate\n",
    "with open('sample_text.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"This is a sample text document.\\nIt contains multiple lines.\\nWe will use this for our NLP pipeline.\")\n",
    "\n",
    "# Load the sample file\n",
    "sample_text = DocumentLoader.load_text_file('sample_text.txt')\n",
    "print(f\"Sample text content:\\n{sample_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919ddb6-3078-4578-931d-4152d297b118",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Now that we can load documents, the next step is to preprocess the text to prepare it for further analysis. Text preprocessing is a critical step in NLP as it transforms raw text into a format that's suitable for machine learning algorithms.\n",
    "\n",
    "### Why Preprocessing Matters\n",
    "\n",
    "Raw text contains various inconsistencies and irrelevant information that can negatively impact model performance. Preprocessing helps to:\n",
    "- Remove noise (irrelevant characters, HTML tags, etc.)\n",
    "- Standardize text (lowercase, removing accents)\n",
    "- Break text into meaningful units (tokens)\n",
    "- Reduce vocabulary size (stemming, lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cb838-8577-4e1c-8815-d3857745d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"A class for preprocessing text data for NLP tasks.\n",
    "    \n",
    "    This class provides methods to clean, normalize, and tokenize text data,\n",
    "    as well as to build vocabulary and convert text to numerical representations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lowercase=True, remove_punctuation=True, remove_numbers=False,\n",
    "                 remove_stopwords=True):\n",
    "        \"\"\"Initialize the TextPreprocessor with specified options.\n",
    "        \n",
    "        Args:\n",
    "            lowercase (bool): Whether to convert text to lowercase\n",
    "            remove_punctuation (bool): Whether to remove punctuation\n",
    "            remove_numbers (bool): Whether to remove numbers\n",
    "            remove_stopwords (bool): Whether to remove common stopwords\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "        # English stopwords - common words that often don't add meaning\n",
    "        # In a full implementation, you might load these from a file\n",
    "        self.stopwords = set([\n",
    "            'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "            'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "            'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "            'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "            'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
    "            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
    "            'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now'\n",
    "        ])\n",
    "        \n",
    "        # We'll build vocabulary as we process text\n",
    "        self.vocabulary = {}\n",
    "        self.inverse_vocabulary = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by applying various filtering operations.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to clean\n",
    "            \n",
    "        Returns:\n",
    "            str: The cleaned text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase if specified\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Remove URLs (simple pattern)\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Split text into tokens (words).\n",
    "        \n",
    "        This is a simple whitespace tokenizer. A more advanced implementation\n",
    "        might handle hyphenated words, contractions, etc. better.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of tokens\n",
    "        \"\"\"\n",
    "        # Simple whitespace tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords if specified\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stopwords]\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def stem_token(self, token):\n",
    "        \"\"\"Implement a very basic stemming algorithm.\n",
    "        \n",
    "        This is a simplified implementation of stemming that removes common \n",
    "        English suffixes. A real implementation would use Porter stemmer or similar.\n",
    "        \n",
    "        Args:\n",
    "            token (str): The token to stem\n",
    "            \n",
    "        Returns:\n",
    "            str: The stemmed token\n",
    "        \"\"\"\n",
    "        # Simple suffix removal - extremely simplified stemming\n",
    "        if len(token) > 3:  # Only stem if token is long enough\n",
    "            if token.endswith('ing'):\n",
    "                return token[:-3]\n",
    "            elif token.endswith('ed'):\n",
    "                return token[:-2]\n",
    "            elif token.endswith('s') and not token.endswith('ss'):\n",
    "                return token[:-1]\n",
    "        return token\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Apply the full preprocessing pipeline to text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The raw input text\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of preprocessed tokens\n",
    "        \"\"\"\n",
    "        # Clean the text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        \n",
    "        # Apply stemming - optional, depending on your needs\n",
    "        stemmed_tokens = [self.stem_token(token) for token in tokens]\n",
    "        \n",
    "        return stemmed_tokens\n",
    "    \n",
    "    def build_vocabulary(self, documents, max_vocab_size=None):\n",
    "        \"\"\"Build vocabulary from a collection of documents.\n",
    "        \n",
    "        Args:\n",
    "            documents (list or dict): Collection of text documents to process\n",
    "            max_vocab_size (int, optional): Maximum vocabulary size. If None, all tokens are kept.\n",
    "            \n",
    "        Returns:\n",
    "            dict: The vocabulary mapping tokens to indices\n",
    "        \"\"\"\n",
    "        # Counter for word frequencies\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Process all documents\n",
    "        if isinstance(documents, dict):\n",
    "            # If documents is a dictionary (like what DocumentLoader.load_directory returns)\n",
    "            doc_list = list(documents.values())\n",
    "        else:\n",
    "            # If documents is already a list\n",
    "            doc_list = documents\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for doc in doc_list:\n",
    "            tokens = self.preprocess(doc)\n",
    "            for token in tokens:\n",
    "                if token in word_counts:\n",
    "                    word_counts[token] += 1\n",
    "                else:\n",
    "                    word_counts[token] = 1\n",
    "        \n",
    "        # Sort words by frequency (most common first)\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Limit vocabulary size if specified\n",
    "        if max_vocab_size is not None:\n",
    "            sorted_words = sorted_words[:max_vocab_size]\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        # We add special tokens: <PAD> for padding sequences, <UNK> for unknown words\n",
    "        self.vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for idx, (word, _) in enumerate(sorted_words, start=2):  # Start from 2 because 0 and 1 are special tokens\n",
    "            self.vocabulary[word] = idx\n",
    "        \n",
    "        # Create inverse vocabulary (index to word)\n",
    "        self.inverse_vocabulary = {idx: word for word, idx in self.vocabulary.items()}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        return self.vocabulary\n",
    "    \n",
    "    def text_to_sequence(self, text, max_length=None):\n",
    "        \"\"\"Convert a text string to a sequence of indices using the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text\n",
    "            max_length (int, optional): Maximum sequence length. If specified, longer sequences\n",
    "                                        are truncated and shorter ones are padded.\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of integers representing the text\n",
    "        \"\"\"\n",
    "        if not self.vocabulary:\n",
    "            raise ValueError(\"Vocabulary hasn't been built yet. Call build_vocabulary() first.\")\n",
    "        \n",
    "        # Preprocess text\n",
    "        tokens = self.preprocess(text)\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        sequence = [self.vocabulary.get(token, self.vocabulary['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Handle sequence length if specified\n",
    "        if max_length is not None:\n",
    "            # Truncate if too long\n",
    "            if len(sequence) > max_length:\n",
    "                sequence = sequence[:max_length]\n",
    "            # Pad if too short\n",
    "            elif len(sequence) < max_length:\n",
    "                sequence = sequence + [self.vocabulary['<PAD>']] * (max_length - len(sequence))\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence):\n",
    "        \"\"\"Convert a sequence of indices back to text.\n",
    "        \n",
    "        Args:\n",
    "            sequence (list): A sequence of indices\n",
    "            \n",
    "        Returns:\n",
    "            str: The reconstructed text\n",
    "        \"\"\"\n",
    "        if not self.inverse_vocabulary:\n",
    "            raise ValueError(\"Vocabulary hasn't been built yet. Call build_vocabulary() first.\")\n",
    "        \n",
    "        # Convert indices to tokens\n",
    "        tokens = [self.inverse_vocabulary.get(idx, '<UNK>') for idx in sequence]\n",
    "        \n",
    "        # Remove padding tokens\n",
    "        tokens = [token for token in tokens if token != '<PAD>']\n",
    "        \n",
    "        # Join tokens to form text\n",
    "        return ' '.join(tokens)\n",
    "        \n",
    "    def display_colored_tokens(self, text):\n",
    "        \"\"\"Display tokens with different colors for better visualization.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to tokenize and display\n",
    "            \n",
    "        Returns:\n",
    "            HTML: Colorful display of tokens\n",
    "        \"\"\"\n",
    "        # Clean the text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        \n",
    "        # Define a set of colors for visualization\n",
    "        colors = [\n",
    "            \"#FF5733\", \"#33FF57\", \"#3357FF\", \"#F3FF33\", \"#FF33F3\",\n",
    "            \"#33FFF3\", \"#F333FF\", \"#FF9933\", \"#33FF99\", \"#9933FF\"\n",
    "        ]\n",
    "        \n",
    "        # Create HTML for colored tokens\n",
    "        html = \"\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            color = colors[i % len(colors)]\n",
    "            html += f'<span style=\"background-color: {color}; padding: 2px 5px; margin: 2px; border-radius: 5px;\">{token}</span> '\n",
    "        \n",
    "        # Display the HTML\n",
    "        return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe9909-ecd7-4d4e-b20d-49708050e2b4",
   "metadata": {},
   "source": [
    "### Visual Tokenization\n",
    "\n",
    "One of the most important steps in NLP is tokenization - breaking text into meaningful units (usually words). Let's visualize this process with colorful highlighting so you can easily see how text is split into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca83386-3a82-4a87-b9d8-a07c2053550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from sample documents\n",
    "preprocessor = TextPreprocessor()\n",
    "sample_docs = [\n",
    "    \"Natural language processing (NLP) is a subfield of linguistics and computer science.\",\n",
    "    \"Machine learning algorithms can process large amounts of natural language data.\",\n",
    "    \"Deep learning models have transformed how computers understand human language.\"\n",
    "]\n",
    "\n",
    "# Build the vocabulary\n",
    "preprocessor.build_vocabulary(sample_docs)\n",
    "\n",
    "# Display the vocabulary as a table\n",
    "vocab_df = pd.DataFrame([\n",
    "    {\"Word\": word, \"ID\": idx} \n",
    "    for word, idx in preprocessor.vocabulary.items()\n",
    "]).sort_values(\"ID\")\n",
    "\n",
    "print(\"Vocabulary (Word-to-ID Mapping):\")\n",
    "display(vocab_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc3806-f72e-4c52-878b-2bfab17ddfe2",
   "metadata": {},
   "source": [
    "### Converting Text to Numerical Sequences\n",
    "\n",
    "Now we can convert any text into a sequence of numbers (word IDs) using our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc355692-30d0-4040-8b31-ff701220ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a text to a sequence of indices\n",
    "new_text = \"Natural language processing helps computers understand human language.\"\n",
    "sequence = preprocessor.text_to_sequence(new_text)\n",
    "\n",
    "# Create a visualization showing each word and its corresponding ID\n",
    "tokens = preprocessor.preprocess(new_text)\n",
    "token_ids = [preprocessor.vocabulary.get(token, preprocessor.vocabulary['<UNK>']) for token in tokens]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "sequence_df = pd.DataFrame({\n",
    "    \"Position\": range(1, len(tokens) + 1),\n",
    "    \"Token\": tokens,\n",
    "    \"Token ID\": token_ids\n",
    "})\n",
    "\n",
    "print(f\"Converting text to numerical sequence:\")\n",
    "print(f\"Original text: \\\"{new_text}\\\"\")\n",
    "display(sequence_df)\n",
    "\n",
    "# Visualize with colors\n",
    "colors = sns.color_palette(\"husl\", len(tokens)).as_hex()\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "    plt.text(i, 0, f\"{token}\\n(ID: {token_id})\", \n",
    "             ha='center', va='center', \n",
    "             bbox=dict(facecolor=colors[i], alpha=0.7, boxstyle='round,pad=0.5'))\n",
    "plt.xlim(-0.5, len(tokens) - 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.axis('off')\n",
    "plt.title(\"Text to Numerical Sequence Visualization\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b4712-ab99-4b58-8a35-c975e8db0ae7",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering: Creating Word Embeddings\n",
    "\n",
    "Now that we can convert words to numbers, we need to create richer representations called \"word embeddings\". \n",
    "These are dense vectors that capture semantic meanings and relationships between words.\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings map words to vectors of real numbers in a way that words with similar meanings have similar vectors. This allows our model to understand semantic relationships between words.\n",
    "\n",
    "Let's implement a simple word embedding method called \"one-hot encoding\" first, and then a more advanced method that learns from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c527b-c9ee-4ae6-9d81-41137099d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    \"\"\"A class that provides different word embedding methods.\n",
    "    \n",
    "    Word embeddings are vector representations of words that capture semantic relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor):\n",
    "        \"\"\"Initialize with a preprocessor that contains vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            preprocessor (TextPreprocessor): A preprocessor with a built vocabulary\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        if not preprocessor.vocabulary:\n",
    "            raise ValueError(\"Preprocessor must have a built vocabulary. Call build_vocabulary() first.\")\n",
    "        \n",
    "        self.vocab_size = preprocessor.vocab_size\n",
    "        self.embedding_matrix = None\n",
    "        self.embedding_dim = None\n",
    "    \n",
    "    def create_one_hot_encodings(self):\n",
    "        \"\"\"Create one-hot encoding representations for each word.\n",
    "        \n",
    "        One-hot encoding represents each word as a vector with a 1 at the word's\n",
    "        index position and 0s everywhere else.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Matrix of one-hot encodings, shape (vocab_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Each word is a vector with all 0s and a single 1\n",
    "        self.embedding_dim = self.vocab_size\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.vocab_size))\n",
    "        \n",
    "        # Set 1 at the corresponding position for each word\n",
    "        for i in range(self.vocab_size):\n",
    "            self.embedding_matrix[i, i] = 1\n",
    "            \n",
    "        return self.embedding_matrix\n",
    "        \n",
    "    def create_random_embeddings(self, embedding_dim=50, seed=42):\n",
    "        \"\"\"Create random embedding vectors for each word.\n",
    "        \n",
    "        Random embeddings are a simple starting point before training.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionality of the embedding vectors\n",
    "            seed (int): Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Matrix of random embeddings, shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_matrix = np.random.normal(size=(self.vocab_size, embedding_dim))\n",
    "        \n",
    "        # Normalize vectors to unit length\n",
    "        norms = np.sqrt((self.embedding_matrix ** 2).sum(axis=1, keepdims=True))\n",
    "        self.embedding_matrix = self.embedding_matrix / norms\n",
    "        \n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        \"\"\"Get the embedding vector for a specific word.\n",
    "        \n",
    "        Args:\n",
    "            word (str): The word to get an embedding for\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: The embedding vector\n",
    "        \"\"\"\n",
    "        if self.embedding_matrix is None:\n",
    "            raise ValueError(\"Embeddings haven't been created yet.\")\n",
    "            \n",
    "        word_idx = self.preprocessor.vocabulary.get(word, self.preprocessor.vocabulary['<UNK>'])\n",
    "        return self.embedding_matrix[word_idx]\n",
    "    \n",
    "    def get_sequence_embeddings(self, text):\n",
    "        \"\"\"Convert a text string to a sequence of embedding vectors.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Matrix of embedding vectors, shape (sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        if self.embedding_matrix is None:\n",
    "            raise ValueError(\"Embeddings haven't been created yet.\")\n",
    "            \n",
    "        # Get sequence of word indices\n",
    "        sequence = self.preprocessor.text_to_sequence(text)\n",
    "        \n",
    "        # Convert to embeddings\n",
    "        embeddings = np.zeros((len(sequence), self.embedding_dim))\n",
    "        for i, word_idx in enumerate(sequence):\n",
    "            embeddings[i] = self.embedding_matrix[word_idx]\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def visualize_embeddings_table(self, words=None, n_words=10):\n",
    "        \"\"\"Visualize word embeddings in a tabular format for easy understanding.\n",
    "        \n",
    "        Args:\n",
    "            words (list, optional): Specific words to visualize\n",
    "            n_words (int): Number of words to display if words is None\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Table of word embeddings\n",
    "        \"\"\"\n",
    "        if self.embedding_matrix is None:\n",
    "            raise ValueError(\"Embeddings haven't been created yet.\")\n",
    "            \n",
    "        if words is None:\n",
    "            # Take first n_words from vocabulary, excluding special tokens\n",
    "            words = list(self.preprocessor.vocabulary.keys())[2:2+n_words]\n",
    "        \n",
    "        # Create a DataFrame with each word and its embedding values\n",
    "        embedding_data = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_idx = self.preprocessor.vocabulary.get(word, self.preprocessor.vocabulary['<UNK>'])\n",
    "            embedding = self.embedding_matrix[word_idx]\n",
    "            \n",
    "            # If embedding dimension is large, only show first few values\n",
    "            display_dim = min(10, self.embedding_dim)\n",
    "            row_data = {\"Word\": word}\n",
    "            \n",
    "            # Add embedding dimensions as columns\n",
    "            for i in range(display_dim):\n",
    "                row_data[f\"Dim {i+1}\"] = round(float(embedding[i]), 3)\n",
    "                \n",
    "            embedding_data.append(row_data)\n",
    "            \n",
    "        # Create and style the DataFrame\n",
    "        df = pd.DataFrame(embedding_data)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc70a0c-cb58-4b51-bcb6-435f50e67a21",
   "metadata": {},
   "source": [
    "### Visualizing Word Embeddings in a Table Format\n",
    "\n",
    "Word embeddings are arrays of numbers that represent words. Let's look at them in a table format that's easy to understand even if you're familiar only with Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612a5ce-90ae-407b-870b-dc2ac8a60bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embeddings using our vocabulary\n",
    "embedding = WordEmbedding(preprocessor)\n",
    "\n",
    "# Generate random embeddings (dimension = 10 for easier visualization)\n",
    "embedding.create_random_embeddings(embedding_dim=10)\n",
    "\n",
    "# Display embeddings in a table format (Excel-like)\n",
    "print(\"Word Embeddings Table (Each word is represented as a vector of numbers):\")\n",
    "embedding_table = embedding.visualize_embeddings_table(n_words=8)\n",
    "display(embedding_table)\n",
    "\n",
    "# Format the table to highlight positive and negative values with colors\n",
    "def style_embeddings(df):\n",
    "    # Make a copy of the dataframe\n",
    "    styled_df = df.copy()\n",
    "    numeric_columns = styled_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Apply styling using Pandas and CSS\n",
    "    styled = styled_df.style.background_gradient(cmap='coolwarm', subset=numeric_columns)\n",
    "    return styled\n",
    "\n",
    "display(style_embeddings(embedding_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f70d66-f5e1-4b4c-9089-dfe118ffc18b",
   "metadata": {},
   "source": [
    "### Creating a Simple Co-occurrence Matrix\n",
    "\n",
    "Another way to create word embeddings is by analyzing how often words appear together. This method creates a co-occurrence matrix where each cell represents how often two words appear in the same context (e.g., same sentence).\n",
    "\n",
    "This approach is similar to the foundation of methods like GloVe (Global Vectors for Word Representation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9957137-3b99-4769-9f2b-46eb94b6df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceEmbedding:\n",
    "    \"\"\"A class that creates word embeddings based on co-occurrence statistics.\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor, window_size=2):\n",
    "        \"\"\"Initialize with a preprocessor and window size.\n",
    "        \n",
    "        Args:\n",
    "            preprocessor (TextPreprocessor): A preprocessor with a built vocabulary\n",
    "            window_size (int): Size of context window (how many words to consider on each side)\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        self.window_size = window_size\n",
    "        self.cooccurrence_matrix = None\n",
    "        \n",
    "    def build_cooccurrence_matrix(self, documents):\n",
    "        \"\"\"Build a word co-occurrence matrix from documents.\n",
    "        \n",
    "        Args:\n",
    "            documents (list): List of documents to analyze\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Co-occurrence matrix\n",
    "        \"\"\"\n",
    "        vocab_size = self.preprocessor.vocab_size\n",
    "        self.cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in documents:\n",
    "            # Get token IDs for the document\n",
    "            tokens = self.preprocessor.preprocess(doc)\n",
    "            token_ids = [self.preprocessor.vocabulary.get(token, self.preprocessor.vocabulary['<UNK>']) \n",
    "                      for token in tokens]\n",
    "            \n",
    "            # Scan the document with a sliding window\n",
    "            for i, target_id in enumerate(token_ids):\n",
    "                # Look at words within the window\n",
    "                window_start = max(0, i - self.window_size)\n",
    "                window_end = min(len(token_ids), i + self.window_size + 1)\n",
    "                \n",
    "                for j in range(window_start, window_end):\n",
    "                    if i != j:  # Skip the word itself\n",
    "                        context_id = token_ids[j]\n",
    "                        self.cooccurrence_matrix[target_id, context_id] += 1\n",
    "        \n",
    "        return self.cooccurrence_matrix\n",
    "    \n",
    "    def visualize_cooccurrence(self, top_n=10):\n",
    "        \"\"\"Visualize co-occurrence matrix for the most frequent words.\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): Number of top words to include\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: Heatmap visualization\n",
    "        \"\"\"\n",
    "        if self.cooccurrence_matrix is None:\n",
    "            raise ValueError(\"Co-occurrence matrix hasn't been built yet.\")\n",
    "        \n",
    "        # Get the most frequent words (excluding special tokens)\n",
    "        word_ids = list(range(2, min(top_n + 2, self.preprocessor.vocab_size)))\n",
    "        words = [self.preprocessor.inverse_vocabulary[idx] for idx in word_ids]\n",
    "        \n",
    "        # Extract submatrix for these words\n",
    "        submatrix = self.cooccurrence_matrix[word_ids, :][:, word_ids]\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(submatrix, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=words, yticklabels=words)\n",
    "        plt.title(f'Word Co-occurrence Matrix (Window Size: {self.window_size})')\n",
    "        plt.ylabel('Target Word')\n",
    "        plt.xlabel('Context Word')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def visualize_cooccurrence_table(self, top_n=8):\n",
    "        \"\"\"Visualize co-occurrence as a table for easier understanding.\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): Number of top words to include\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Table of co-occurrence counts\n",
    "        \"\"\"\n",
    "        if self.cooccurrence_matrix is None:\n",
    "            raise ValueError(\"Co-occurrence matrix hasn't been built yet.\")\n",
    "        \n",
    "        # Get the most frequent words (excluding special tokens)\n",
    "        word_ids = list(range(2, min(top_n + 2, self.preprocessor.vocab_size)))\n",
    "        words = [self.preprocessor.inverse_vocabulary[idx] for idx in word_ids]\n",
    "        \n",
    "        # Extract submatrix for these words\n",
    "        submatrix = self.cooccurrence_matrix[word_ids, :][:, word_ids]\n",
    "        # Create DataFrame for tabular display\n",
    "        cooccurrence_df = pd.DataFrame(submatrix, index=words, columns=words)\n",
    "        \n",
    "        return cooccurrence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bf825-ebfb-4d1d-bcaf-d6634ebd0421",
   "metadata": {},
   "source": [
    "### Visualizing Word Co-occurrences\n",
    "\n",
    "Let's see how often words appear together in our sample text. This matrix shows the count of how many times each word pair appears near each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6225e8e-1141-494d-9256-50b9b1eb8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a co-occurrence embedding instance\n",
    "cooccurrence = CoOccurrenceEmbedding(preprocessor, window_size=2)\n",
    "cooccurrence.build_cooccurrence_matrix(sample_docs)\n",
    "\n",
    "# Display co-occurrence as a table\n",
    "print(\"Word Co-occurrence Table:\")\n",
    "cooccur_table = cooccurrence.visualize_cooccurrence_table(top_n=6)\n",
    "display(cooccur_table.style.background_gradient(cmap='Blues'))\n",
    "\n",
    "# Also visualize as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cooccur_table, annot=True, fmt='.1f', cmap='Blues',\n",
    "           xticklabels=cooccur_table.index, yticklabels=cooccur_table.columns)\n",
    "plt.title(f'Word Co-occurrence Matrix (Window Size: {cooccurrence.window_size})')\n",
    "plt.ylabel('Target Word')\n",
    "plt.xlabel('Context Word')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925d4ed-28d0-4e09-b63d-27922a18f39e",
   "metadata": {},
   "source": [
    "## 4. Building a Neural Network from Scratch\n",
    "\n",
    "Now let's implement a simple neural network using only NumPy. We'll create a feed-forward neural network with the following components:\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer with non-linear activation\n",
    "3. Output layer with softmax activation for classification\n",
    "\n",
    "This network can be used for text classification tasks like sentiment analysis or topic categorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664399c8-d683-4a89-8906-8ec89701c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A simple neural network implemented with NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize neural network architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            hidden_size (int): Size of hidden layer\n",
    "            output_size (int): Number of output classes\n",
    "        \"\"\"\n",
    "        # Initialize with random weights\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Store parameters in a dictionary for easy access\n",
    "        self.parameters = {\n",
    "            'W1': self.W1, 'b1': self.b1,\n",
    "            'W2': self.W2, 'b2': self.b2\n",
    "        }\n",
    "        \n",
    "        # For storing intermediate values during forward pass\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation function: max(0, Z)\n",
    "        \n",
    "        Args:\n",
    "            Z (numpy.ndarray): Input to activation function\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Activated values\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"Derivative of ReLU function.\n",
    "        \n",
    "        Args:\n",
    "            Z (numpy.ndarray): Input to activation function\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Derivatives\n",
    "        \"\"\"\n",
    "        return (Z > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax activation function.\n",
    "        \n",
    "        Args:\n",
    "            Z (numpy.ndarray): Input to activation function\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Probabilities that sum to 1\n",
    "        \"\"\"\n",
    "        # Shift Z for numerical stability\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input features, shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Output probabilities, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # First layer\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Second layer\n",
    "        Z2 = np.dot(A1, self.W2) + self.b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        \n",
    "        # Store values for backpropagation\n",
    "        self.cache = {\n",
    "            'X': X, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "        }\n",
    "        \n",
    "        return A2\n",
    "    \n",
    "    def compute_loss(self, Y_pred, Y_true):\n",
    "        \"\"\"Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            Y_pred (numpy.ndarray): Predicted probabilities, shape (batch_size, output_size)\n",
    "            Y_true (numpy.ndarray): One-hot encoded true labels, shape (batch_size, output_size)\n",
    "            \n",
    "        Returns:\n",
    "            float: Average loss\n",
    "        \"\"\"\n",
    "        # Small value to avoid log(0)\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(Y_true * np.log(Y_pred + epsilon)) / Y_true.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y_true, learning_rate=0.01):\n",
    "        \"\"\"Backward pass to update weights.\n",
    "        \n",
    "        Args:\n",
    "            Y_true (numpy.ndarray): One-hot encoded true labels\n",
    "            learning_rate (float): Learning rate for gradient descent\n",
    "            \n",
    "        Returns:\n",
    "            dict: Gradients of parameters\n",
    "        \"\"\"\n",
    "        m = Y_true.shape[0]  # Batch size\n",
    "        \n",
    "        # Get predictions from cache\n",
    "        A2 = self.cache['A2']\n",
    "        A1 = self.cache['A1']\n",
    "        X = self.cache['X']\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = A2 - Y_true\n",
    "        dW2 = np.dot(A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(self.cache['Z1'])\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Store gradients\n",
    "        gradients = {\n",
    "            'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        \n",
    "        self.parameters = {\n",
    "            'W1': self.W1, 'b1': self.b1,\n",
    "            'W2': self.W2, 'b2': self.b2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Training features\n",
    "            Y (numpy.ndarray): Training labels (one-hot encoded)\n",
    "            epochs (int): Number of training iterations\n",
    "            learning_rate (float): Learning rate for gradient descent\n",
    "            verbose (bool): Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            list: Training losses\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            Y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y_pred, Y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(Y, learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for new data.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input features\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class indices\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        probabilities = self.forward(X)\n",
    "        \n",
    "        # Get class with highest probability\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def visualize_network(self):\n",
    "        \"\"\"Visualize the network architecture in a tabular format.\"\"\"\n",
    "        # Create a DataFrame showing layer structure\n",
    "        layers = [\n",
    "            {\"Layer\": \"Input\", \"Neurons\": self.input_size, \"Activation\": \"None\"},\n",
    "            {\"Layer\": \"Hidden\", \"Neurons\": self.hidden_size, \"Activation\": \"ReLU\"},\n",
    "            {\"Layer\": \"Output\", \"Neurons\": self.output_size, \"Activation\": \"Softmax\"}\n",
    "        ]\n",
    "        \n",
    "        network_df = pd.DataFrame(layers)\n",
    "        return network_df\n",
    "    \n",
    "    def visualize_weights(self, layer=1, max_rows=8, max_cols=8):\n",
    "        \"\"\"Visualize weights as a table for better understanding.\n",
    "        \n",
    "        Args:\n",
    "            layer (int): Which layer's weights to visualize (1 or 2)\n",
    "            max_rows (int): Maximum number of input neurons to show\n",
    "            max_cols (int): Maximum number of output neurons to show\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Table of weights\n",
    "        \"\"\"\n",
    "        if layer == 1:\n",
    "            W = self.W1\n",
    "            layer_name = \"Hidden Layer Weights\"\n",
    "            in_prefix = \"Input\"\n",
    "            out_prefix = \"Hidden\"\n",
    "        else:\n",
    "            W = self.W2\n",
    "            layer_name = \"Output Layer Weights\"\n",
    "            in_prefix = \"Hidden\"\n",
    "            out_prefix = \"Output\"\n",
    "        \n",
    "        # Limit dimensions for display\n",
    "        rows = min(W.shape[0], max_rows)\n",
    "        cols = min(W.shape[1], max_cols)\n",
    "        \n",
    "        # Create row and column labels\n",
    "        row_labels = [f\"{in_prefix} {i+1}\" for i in range(rows)]\n",
    "        col_labels = [f\"{out_prefix} {j+1}\" for j in range(cols)]\n",
    "        \n",
    "        # Create DataFrame with weight values\n",
    "        weights_df = pd.DataFrame(W[:rows, :cols], index=row_labels, columns=col_labels)\n",
    "        \n",
    "        return weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3056369-a941-49b4-a4fc-fbcaa8ed7fd3",
   "metadata": {},
   "source": [
    "### Visualizing the Neural Network\n",
    "\n",
    "Let's visualize our neural network architecture and weights so they're easy to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dac703-ccf9-4a68-a7a4-58c7ca172f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neural network for a text classification task\n",
    "# Let's say we have 10-dimensional word embeddings and 3 output classes\n",
    "nn = NeuralNetwork(input_size=10, hidden_size=5, output_size=3)\n",
    "\n",
    "# Visualize the network architecture\n",
    "print(\"Neural Network Architecture:\")\n",
    "display(nn.visualize_network())\n",
    "\n",
    "# Visualize the weights between layers\n",
    "print(\"\\nHidden Layer Weights (connecting input to hidden neurons):\")\n",
    "hidden_weights = nn.visualize_weights(layer=1)\n",
    "display(hidden_weights.style.background_gradient(cmap='coolwarm'))\n",
    "\n",
    "print(\"\\nOutput Layer Weights (connecting hidden to output neurons):\")\n",
    "output_weights = nn.visualize_weights(layer=2)\n",
    "display(output_weights.style.background_gradient(cmap='coolwarm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccc168-ec27-4cc5-9e1d-52b92c9dcbdb",
   "metadata": {},
   "source": [
    "## 5. Text Classification Example\n",
    "\n",
    "Now let's put everything together to create a text classifier. We'll implement a simple sentiment analysis model that classifies text as positive, negative, or neutral.\n",
    "\n",
    "Here's our complete pipeline:\n",
    "1. Load and preprocess text data\n",
    "2. Convert text to embeddings\n",
    "3. Train the neural network\n",
    "4. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d673f-ba41-4152-b540-8d4dd828739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \"\"\"A text classifier that combines preprocessing, embeddings, and a neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=50, hidden_size=20):\n",
    "        \"\"\"Initialize the text classifier.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionality of word embeddings\n",
    "            hidden_size (int): Size of hidden layer in neural network\n",
    "        \"\"\"\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.embedding = None\n",
    "        self.neural_network = None\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classes = []\n",
    "        \n",
    "    def prepare_data(self, texts, labels):\n",
    "        \"\"\"Prepare text data for classification.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of text documents\n",
    "            labels (list): List of corresponding labels\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X, Y) prepared features and one-hot encoded labels\n",
    "        \"\"\"\n",
    "        # Build vocabulary from texts\n",
    "        self.preprocessor.build_vocabulary(texts)\n",
    "        \n",
    "        # Create embeddings\n",
    "        self.embedding = WordEmbedding(self.preprocessor)\n",
    "        self.embedding.create_random_embeddings(embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        # Get unique classes\n",
    "        self.classes = sorted(set(labels))\n",
    "        num_classes = len(self.classes)\n",
    "        \n",
    "        # Create neural network\n",
    "        if self.neural_network is None:\n",
    "            self.neural_network = NeuralNetwork(\n",
    "                input_size=self.embedding_dim,\n",
    "                hidden_size=self.hidden_size,\n",
    "                output_size=num_classes\n",
    "            )\n",
    "        \n",
    "        # Prepare features (document embeddings)\n",
    "        X = np.zeros((len(texts), self.embedding_dim))\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            # Convert text to sequence of embedding vectors\n",
    "            embeddings = self.embedding.get_sequence_embeddings(text)\n",
    "            \n",
    "            # Average the embeddings to get a document vector\n",
    "            X[i] = np.mean(embeddings, axis=0)\n",
    "        \n",
    "        # Prepare labels (one-hot encoding)\n",
    "        Y = np.zeros((len(labels), num_classes))\n",
    "        for i, label in enumerate(labels):\n",
    "            class_idx = self.classes.index(label)\n",
    "            Y[i, class_idx] = 1\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    def train(self, texts, labels, epochs=500, learning_rate=0.01):\n",
    "        \"\"\"Train the classifier.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Training texts\n",
    "            labels (list): Training labels\n",
    "            epochs (int): Number of training iterations\n",
    "            learning_rate (float): Learning rate for gradient descent\n",
    "            \n",
    "        Returns:\n",
    "            list: Training losses\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        X, Y = self.prepare_data(texts, labels)\n",
    "        \n",
    "        # Train neural network\n",
    "        losses = self.neural_network.train(X, Y, epochs=epochs, \n",
    "                                         learning_rate=learning_rate, \n",
    "                                         verbose=True)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict the class of a text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            str: Predicted class label\n",
    "        \"\"\"\n",
    "        # Convert text to embedding\n",
    "        embeddings = self.embedding.get_sequence_embeddings(text)\n",
    "        X = np.mean(embeddings, axis=0).reshape(1, -1)\n",
    "        \n",
    "        # Get prediction\n",
    "        class_idx = self.neural_network.predict(X)[0]\n",
    "        return self.classes[class_idx]\n",
    "    \n",
    "    def predict_with_explanation(self, text):\n",
    "        \"\"\"Predict with a more detailed explanation for non-technical users.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            dict: Prediction results with explanation\n",
    "        \"\"\"\n",
    "        # Step 1: Preprocess text\n",
    "        tokens = self.preprocessor.preprocess(text)\n",
    "        \n",
    "        # Step 2: Get token IDs\n",
    "        token_ids = [self.preprocessor.vocabulary.get(token, self.preprocessor.vocabulary['<UNK>']) \n",
    "                     for token in tokens]\n",
    "        \n",
    "        # Step 3: Get embeddings\n",
    "        embeddings = self.embedding.get_sequence_embeddings(text)\n",
    "        document_vector = np.mean(embeddings, axis=0)\n",
    "        \n",
    "        # Step 4: Make prediction\n",
    "        X = document_vector.reshape(1, -1)\n",
    "        probabilities = self.neural_network.forward(X)[0]\n",
    "        predicted_idx = np.argmax(probabilities)\n",
    "        predicted_class = self.classes[predicted_idx]\n",
    "        \n",
    "        # Prepare explanation\n",
    "        explanation = {\n",
    "            \"text\": text,\n",
    "            \"tokens\": tokens,\n",
    "            \"predicted_class\": predicted_class,\n",
    "            \"confidence\": float(probabilities[predicted_idx]),\n",
    "            \"class_probabilities\": {\n",
    "                self.classes[i]: float(prob) \n",
    "                for i, prob in enumerate(probabilities)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "        \n",
    "    def visualize_prediction(self, text):\n",
    "        \"\"\"Visualize the prediction process in a way non-technical users can understand.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to classify\n",
    "            \n",
    "        Returns:\n",
    "            None: Displays visualizations\n",
    "        \"\"\"\n",
    "        # Get prediction with explanation\n",
    "        results = self.predict_with_explanation(text)\n",
    "        \n",
    "        # Display original text\n",
    "        print(f\"Original text: \\\"{text}\\\"\")\n",
    "        \n",
    "        # Display tokenization\n",
    "        print(\"\\nStep 1: Breaking text into tokens (words)\")\n",
    "        display(self.preprocessor.display_colored_tokens(text))\n",
    "        \n",
    "        # Display word-to-number conversion\n",
    "        print(\"\\nStep 2: Converting tokens to numbers using our vocabulary\")\n",
    "        tokens = self.preprocessor.preprocess(text)\n",
    "        token_ids = [self.preprocessor.vocabulary.get(token, self.preprocessor.vocabulary['<UNK>']) \n",
    "                    for token in tokens]\n",
    "        \n",
    "        tokens_df = pd.DataFrame({\n",
    "            \"Token\": tokens,\n",
    "            \"Token ID\": token_ids\n",
    "        })\n",
    "        display(tokens_df)\n",
    "        \n",
    "        # Display prediction results\n",
    "        print(\"\\nStep 3: Making a prediction using our neural network\")\n",
    "        \n",
    "        # Create a DataFrame for classification results\n",
    "        results_df = pd.DataFrame([\n",
    "            {\"Class\": class_name, \"Probability\": f\"{prob:.2%}\"}\n",
    "            for class_name, prob in results[\"class_probabilities\"].items()\n",
    "        ]).sort_values(\"Probability\", ascending=False)\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"\\nPrediction: {results['predicted_class']} with {results['confidence']:.2%} confidence\")\n",
    "        display(results_df)\n",
    "        \n",
    "        # Visualize probabilities as a bar chart\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        ax = sns.barplot(x=list(results[\"class_probabilities\"].keys()), \n",
    "                        y=list(results[\"class_probabilities\"].values()))\n",
    "        plt.title(\"Classification Probabilities\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        \n",
    "        # Add percentage labels on top of bars\n",
    "        for i, p in enumerate(ax.patches):\n",
    "            ax.annotate(f\"{p.get_height():.2%}\", \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha = 'center', va = 'bottom')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e356c4b-59e1-48d6-9660-4b284e13ad41",
   "metadata": {},
   "source": [
    "### Demonstration: Sentiment Analysis\n",
    "\n",
    "Let's create a simple sentiment classifier using our from-scratch NLP pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb7595-3801-438e-86dc-5003cbc1336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentiment analysis data\n",
    "train_texts = [\n",
    "    \"This movie was amazing! I loved every minute of it.\",\n",
    "    \"Great performances by the entire cast. Highly recommended.\",\n",
    "    \"The special effects were incredible and the plot was engaging.\",\n",
    "    \"This film was a complete waste of time. Terrible acting.\",\n",
    "    \"I hated the ending. Very disappointing movie overall.\",\n",
    "    \"Boring plot with predictable twists. Don't bother watching.\",\n",
    "    \"The movie was okay, nothing special but not terrible either.\",\n",
    "    \"Average film with some good moments and some dull parts.\",\n",
    "    \"It was neither great nor awful, just a standard film.\"\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    \"positive\", \"positive\", \"positive\",\n",
    "    \"negative\", \"negative\", \"negative\",\n",
    "    \"neutral\", \"neutral\", \"neutral\"\n",
    "]\n",
    "\n",
    "# Create and train a classifier\n",
    "classifier = TextClassifier(embedding_dim=20, hidden_size=10)\n",
    "losses = classifier.train(train_texts, train_labels, epochs=300)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test with new examples and visualize the complete process\n",
    "test_examples = [\n",
    "    \"I really enjoyed this movie, it was fantastic!\",\n",
    "    \"This was the worst film I've ever seen.\",\n",
    "    \"The movie was fine, I guess.\"\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    classifier.visualize_prediction(example)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552d088-7755-4736-8e93-7a520f94ebae",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've built a complete NLP pipeline from scratch using only Python and NumPy:\n",
    "\n",
    "1. **Document Loading**: We implemented loaders for different document formats including PDF, DOCX, and plain text.\n",
    "\n",
    "2. **Text Preprocessing**: We built a preprocessing pipeline that cleans, tokenizes, and normalizes text data.\n",
    "\n",
    "3. **Feature Engineering**: We created word embeddings to represent text as numerical vectors.\n",
    "\n",
    "4. **Neural Network**: We implemented a feed-forward neural network for text classification.\n",
    "\n",
    "5. **Text Classification**: We built a complete sentiment analysis system that classifies text as positive, negative, or neutral.\n",
    "\n",
    "### Further Improvements\n",
    "\n",
    "There are many ways to enhance this basic implementation:\n",
    "\n",
    "1. **Better Tokenization**: Implement more sophisticated tokenization methods like subword tokenization (BPE, WordPiece)\n",
    "\n",
    "2. **Better Embeddings**: Implement word2vec or GloVe-style embeddings trained on your data\n",
    "\n",
    "3. **Network Architecture**: Add more layers, implement recurrent or transformer architectures\n",
    "\n",
    "4. **Regularization**: Add dropout, batch normalization, or weight decay to prevent overfitting\n",
    "\n",
    "5. **Evaluation**: Implement cross-validation and more evaluation metrics\n",
    "\n",
    "### Remember\n",
    "\n",
    "This implementation is designed for educational purposes to understand how NLP systems work from the ground up. For production use, established libraries like spaCy, Hugging Face Transformers, or TensorFlow/PyTorch would be more efficient and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95609f-e938-428c-959d-0af41aed0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a more diverse training dataset\n",
    "train_texts = [\n",
    "    # Positive examples\n",
    "    \"This movie was absolutely amazing! The acting was superb and the plot was engaging.\",\n",
    "    \"I loved every minute of this film. The special effects were incredible.\",\n",
    "    \"What a fantastic experience! The cinematography was beautiful and the story was touching.\",\n",
    "    \"Highly recommended! The performances were outstanding and the direction was perfect.\",\n",
    "    \n",
    "    # Negative examples\n",
    "    \"This was the worst movie I've ever seen. Terrible acting and a boring plot.\",\n",
    "    \"I hated every minute of this film. The special effects were cheap and unconvincing.\",\n",
    "    \"What a waste of time! The story made no sense and the characters were unlikable.\",\n",
    "    \"Not recommended at all. The performances were wooden and the direction was confusing.\",\n",
    "    \n",
    "    # Neutral examples\n",
    "    \"The movie was okay. Some parts were good, others not so much.\",\n",
    "    \"It was an average film. Nothing special but not terrible either.\",\n",
    "    \"The movie had its moments, but overall it was just decent.\",\n",
    "    \"Neither great nor awful. Just a standard film with some good and bad parts.\"\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    \"positive\", \"positive\", \"positive\", \"positive\",\n",
    "    \"negative\", \"negative\", \"negative\", \"negative\",\n",
    "    \"neutral\", \"neutral\", \"neutral\", \"neutral\"\n",
    "]\n",
    "\n",
    "# Create and train the classifier\n",
    "classifier = TextClassifier(embedding_dim=20, hidden_size=10)\n",
    "print(\"Training the classifier...\")\n",
    "losses = classifier.train(train_texts, train_labels, epochs=500)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test with new examples\n",
    "test_examples = [\n",
    "    \"This film exceeded all my expectations! The acting was phenomenal and the story was captivating.\",\n",
    "    \"I was really disappointed with this movie. The plot was confusing and the acting was subpar.\",\n",
    "    \"The movie was decent. Some scenes were good, others were a bit slow.\",\n",
    "    \"What an incredible experience! The cinematography was breathtaking and the performances were outstanding.\",\n",
    "    \"This was a complete waste of time. The story made no sense and the characters were unlikable.\",\n",
    "    \"The film was okay, but nothing special. The plot was predictable and the acting was average.\"\n",
    "]\n",
    "\n",
    "print(\"\\nMaking predictions on new text examples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for example in test_examples:\n",
    "    # Get prediction with explanation\n",
    "    results = classifier.predict_with_explanation(example)\n",
    "    \n",
    "    # Display results in a clear format\n",
    "    print(f\"\\nText: \\\"{example}\\\"\")\n",
    "    print(f\"Predicted sentiment: {results['predicted_class']}\")\n",
    "    print(\"\\nConfidence scores:\")\n",
    "    for class_name, prob in results['class_probabilities'].items():\n",
    "        print(f\"{class_name}: {prob:.2%}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Let's also visualize one prediction in detail\n",
    "print(\"\\nDetailed visualization of one prediction:\")\n",
    "classifier.visualize_prediction(test_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6f91-0ace-4846-a357-69946f12f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s.]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Character-level dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=20):\n",
    "        self.text = preprocess_text(text)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Create character vocabulary\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Create training samples\n",
    "        self.data = self._create_samples()\n",
    "    \n",
    "    def _create_samples(self):\n",
    "        char_indices = [self.char_to_idx[ch] for ch in self.text]\n",
    "        samples = []\n",
    "        \n",
    "        for i in range(0, len(char_indices) - self.seq_length - 1, 1):\n",
    "            input_seq = char_indices[i:i + self.seq_length]\n",
    "            target_seq = char_indices[i + 1:i + self.seq_length + 1]\n",
    "            samples.append((input_seq, target_seq))\n",
    "            \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.data[idx]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# Simple language model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=64, hidden_size=128, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding: [batch_size, seq_length, embed_size]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out: [batch_size, seq_length, hidden_size]\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Reshape output to [batch_size * seq_length, hidden_size]\n",
    "        out = out.reshape(-1, out.shape[2])\n",
    "        \n",
    "        # Decode hidden states to get output for each time step\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, num_epochs=50, learning_rate=0.001):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device).view(-1)  # Flatten targets\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i+1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Generate text\n",
    "def generate_text(model, dataset, seed_text, length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Process seed text\n",
    "    chars = [ch for ch in seed_text.lower() if ch in dataset.char_to_idx]\n",
    "    \n",
    "    # Pad if needed\n",
    "    while len(chars) < dataset.seq_length:\n",
    "        chars.insert(0, ' ')\n",
    "    \n",
    "    # Keep only the last seq_length characters\n",
    "    chars = chars[-dataset.seq_length:]\n",
    "    \n",
    "    # Convert to indices\n",
    "    with torch.no_grad():\n",
    "        indices = [dataset.char_to_idx[ch] for ch in chars]\n",
    "        current_seq = torch.tensor([indices]).to(device)\n",
    "        \n",
    "        result = seed_text\n",
    "        \n",
    "        # Generate new characters\n",
    "        for _ in range(length):\n",
    "            output = model(current_seq)\n",
    "            \n",
    "            # Get prediction for the last character\n",
    "            output = output[-dataset.seq_length:].view(-1, dataset.vocab_size)[-1]\n",
    "            \n",
    "            # Apply temperature\n",
    "            scaled_output = output / temperature\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probabilities = torch.softmax(scaled_output, dim=0).cpu().numpy()\n",
    "            \n",
    "            # Sample next character\n",
    "            next_idx = np.random.choice(dataset.vocab_size, p=probabilities)\n",
    "            next_char = dataset.idx_to_char[next_idx]\n",
    "            \n",
    "            # Add to result\n",
    "            result += next_char\n",
    "            \n",
    "            # Update current sequence\n",
    "            indices = indices[1:] + [next_idx]\n",
    "            current_seq = torch.tensor([indices]).to(device)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # Sample text\n",
    "    text = \"\"\"\n",
    "    Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "    concerned with the interactions between computers and human language, in particular how to program computers to \n",
    "    process and analyze large amounts of natural language data. The result is a computer capable of \"understanding\" \n",
    "    the contents of documents, including the contextual nuances of the language within them. The technology can then \n",
    "    accurately extract information and insights contained in the documents as well as categorize and organize the \n",
    "    documents themselves.\n",
    "\n",
    "    Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial \n",
    "    intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal \n",
    "    human intervention. Machine learning algorithms build a mathematical model based on sample data, known as \"training \n",
    "    data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
    "\n",
    "    Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning \n",
    "    unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network, \n",
    "    deep learning models can achieve state-of-the-art accuracy, sometimes exceeding human-level performance.\n",
    "\n",
    "    Natural Language Processing combines computer science, artificial intelligence, and computational linguistics to \n",
    "    enable computers to process, understand, and generate human language. NLP applications include machine translation, \n",
    "    sentiment analysis, speech recognition, and text summarization. The field has advanced significantly with deep \n",
    "    learning approaches, particularly with transformer models like BERT and GPT, which have revolutionized how machines \n",
    "    understand and generate text.\n",
    "\n",
    "    Neural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between \n",
    "    vast amounts of data. They are used in a variety of applications in financial services, from forecasting and marketing \n",
    "    research to fraud detection and risk assessment.\n",
    "\n",
    "    The term \"artificial intelligence\" was first coined in 1956, at a conference at Dartmouth College. Since then, AI has gone \n",
    "    through numerous cycles of optimism and disappointment, breakthroughs and setbacks.\n",
    "\n",
    "    Language is a complex system of communication that includes syntax, semantics, phonetics, and pragmatics. Natural Language \n",
    "    Processing aims to address all these aspects to create systems that truly understand human language.\n",
    "\n",
    "    When training language models, one common challenge is overfitting, where the model performs well on training data but \n",
    "    fails to generalize to new, unseen examples. Regularization techniques like dropout and weight decay help mitigate this issue.\n",
    "\n",
    "    Text generation has many practical applications, including content creation, chatbots, automated reporting, and creative \n",
    "    writing assistance. Modern language models can generate coherent, contextually relevant text that is often indistinguishable \n",
    "    from human-written content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyperparameters\n",
    "    seq_length = 25\n",
    "    batch_size = 32\n",
    "    embedding_size = 64\n",
    "    hidden_size = 128\n",
    "    num_layers = 2\n",
    "    learning_rate = 0.002\n",
    "    num_epochs = 50\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CharDataset(text, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleRNN(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        embed_size=embedding_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training language model on {device}...\")\n",
    "    print(f\"Vocabulary size: {dataset.vocab_size}\")\n",
    "    losses = train_model(model, dataloader, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerating text samples:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in [0.5, 0.8, 1.2]:\n",
    "        print(f\"\\nTemperature: {temp}\")\n",
    "        generated = generate_text(model, dataset, \"intelligence\", length=200, temperature=temp)\n",
    "        print(generated)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd98786-ec27-4e6c-ae86-17413c22666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Please select a PDF file...\n",
      "Extracting text from: C:/Users/inare/Downloads/Deep Learning for Dummies.pdf\n",
      "PDF has 76 pages\n",
      "Processing page 1/76...\n",
      "Processing page 11/76...\n",
      "Processing page 21/76...\n",
      "Processing page 31/76...\n",
      "Processing page 41/76...\n",
      "Processing page 51/76...\n",
      "Processing page 61/76...\n",
      "Processing page 71/76...\n",
      "Extracted 124812 characters\n",
      "Creating dataset...\n",
      "Preprocessing text...\n",
      "Preprocessed text length: 122744 characters\n",
      "Vocabulary size: 44 characters\n",
      "Created 122663 training samples\n",
      "Initializing model...\n",
      "Model parameters: 938,540\n",
      "Training on cpu...\n",
      "Epoch [1/30], Step [25/1916], Loss: 2.9416\n",
      "Epoch [1/30], Step [50/1916], Loss: 2.6098\n",
      "Epoch [1/30], Step [75/1916], Loss: 2.4098\n",
      "Epoch [1/30], Step [100/1916], Loss: 2.1609\n",
      "Epoch [1/30], Step [125/1916], Loss: 2.0286\n",
      "Epoch [1/30], Step [150/1916], Loss: 1.9995\n",
      "Epoch [1/30], Step [175/1916], Loss: 1.7992\n",
      "Epoch [1/30], Step [200/1916], Loss: 1.7549\n",
      "Epoch [1/30], Step [225/1916], Loss: 1.6922\n",
      "Epoch [1/30], Step [250/1916], Loss: 1.7122\n",
      "Epoch [1/30], Step [275/1916], Loss: 1.6555\n",
      "Epoch [1/30], Step [300/1916], Loss: 1.6408\n",
      "Epoch [1/30], Step [325/1916], Loss: 1.5089\n",
      "Epoch [1/30], Step [350/1916], Loss: 1.5071\n",
      "Epoch [1/30], Step [375/1916], Loss: 1.4617\n",
      "Epoch [1/30], Step [400/1916], Loss: 1.4552\n",
      "Epoch [1/30], Step [425/1916], Loss: 1.3911\n",
      "Epoch [1/30], Step [450/1916], Loss: 1.2896\n",
      "Epoch [1/30], Step [475/1916], Loss: 1.3607\n",
      "Epoch [1/30], Step [500/1916], Loss: 1.3159\n",
      "Epoch [1/30], Step [525/1916], Loss: 1.3192\n",
      "Epoch [1/30], Step [550/1916], Loss: 1.3244\n",
      "Epoch [1/30], Step [575/1916], Loss: 1.2908\n",
      "Epoch [1/30], Step [600/1916], Loss: 1.3070\n",
      "Epoch [1/30], Step [625/1916], Loss: 1.2699\n",
      "Epoch [1/30], Step [650/1916], Loss: 1.2209\n",
      "Epoch [1/30], Step [675/1916], Loss: 1.2374\n",
      "Epoch [1/30], Step [700/1916], Loss: 1.2600\n",
      "Epoch [1/30], Step [725/1916], Loss: 1.2084\n",
      "Epoch [1/30], Step [750/1916], Loss: 1.1978\n",
      "Epoch [1/30], Step [775/1916], Loss: 1.1740\n",
      "Epoch [1/30], Step [800/1916], Loss: 1.2051\n",
      "Epoch [1/30], Step [825/1916], Loss: 1.1096\n",
      "Epoch [1/30], Step [850/1916], Loss: 1.1177\n",
      "Epoch [1/30], Step [875/1916], Loss: 1.0754\n",
      "Epoch [1/30], Step [900/1916], Loss: 1.1105\n",
      "Epoch [1/30], Step [925/1916], Loss: 1.0747\n",
      "Epoch [1/30], Step [950/1916], Loss: 1.1550\n",
      "Epoch [1/30], Step [975/1916], Loss: 1.1342\n",
      "Epoch [1/30], Step [1000/1916], Loss: 1.1690\n",
      "Epoch [1/30], Step [1025/1916], Loss: 1.0264\n",
      "Epoch [1/30], Step [1050/1916], Loss: 1.0898\n",
      "Epoch [1/30], Step [1075/1916], Loss: 1.1996\n",
      "Epoch [1/30], Step [1100/1916], Loss: 1.0866\n",
      "Epoch [1/30], Step [1125/1916], Loss: 1.0550\n",
      "Epoch [1/30], Step [1150/1916], Loss: 1.0974\n",
      "Epoch [1/30], Step [1175/1916], Loss: 1.1003\n",
      "Epoch [1/30], Step [1200/1916], Loss: 1.0219\n",
      "Epoch [1/30], Step [1225/1916], Loss: 1.0822\n",
      "Epoch [1/30], Step [1250/1916], Loss: 1.0747\n",
      "Epoch [1/30], Step [1275/1916], Loss: 1.0160\n",
      "Epoch [1/30], Step [1300/1916], Loss: 1.1091\n",
      "Epoch [1/30], Step [1325/1916], Loss: 1.0471\n",
      "Epoch [1/30], Step [1350/1916], Loss: 1.0603\n",
      "Epoch [1/30], Step [1375/1916], Loss: 1.0345\n",
      "Epoch [1/30], Step [1400/1916], Loss: 1.0036\n",
      "Epoch [1/30], Step [1425/1916], Loss: 1.0000\n",
      "Epoch [1/30], Step [1450/1916], Loss: 1.0329\n",
      "Epoch [1/30], Step [1475/1916], Loss: 0.9077\n",
      "Epoch [1/30], Step [1500/1916], Loss: 0.9725\n",
      "Epoch [1/30], Step [1525/1916], Loss: 0.9571\n",
      "Epoch [1/30], Step [1550/1916], Loss: 0.9559\n",
      "Epoch [1/30], Step [1575/1916], Loss: 0.9565\n",
      "Epoch [1/30], Step [1600/1916], Loss: 1.0361\n",
      "Epoch [1/30], Step [1625/1916], Loss: 1.0431\n",
      "Epoch [1/30], Step [1650/1916], Loss: 1.0226\n",
      "Epoch [1/30], Step [1675/1916], Loss: 0.8433\n",
      "Epoch [1/30], Step [1700/1916], Loss: 0.9174\n",
      "Epoch [1/30], Step [1725/1916], Loss: 0.9791\n",
      "Epoch [1/30], Step [1750/1916], Loss: 0.9496\n",
      "Epoch [1/30], Step [1775/1916], Loss: 0.9716\n",
      "Epoch [1/30], Step [1800/1916], Loss: 0.9001\n",
      "Epoch [1/30], Step [1825/1916], Loss: 0.8434\n",
      "Epoch [1/30], Step [1850/1916], Loss: 0.9274\n",
      "Epoch [1/30], Step [1875/1916], Loss: 0.9403\n",
      "Epoch [1/30], Step [1900/1916], Loss: 0.9865\n",
      "Epoch [1/30], Loss: 1.2766, Time: 221.84s\n",
      "Epoch [2/30], Step [25/1916], Loss: 0.8761\n",
      "Epoch [2/30], Step [50/1916], Loss: 0.9104\n",
      "Epoch [2/30], Step [75/1916], Loss: 0.8784\n",
      "Epoch [2/30], Step [100/1916], Loss: 0.8658\n",
      "Epoch [2/30], Step [125/1916], Loss: 0.8643\n",
      "Epoch [2/30], Step [150/1916], Loss: 0.9189\n",
      "Epoch [2/30], Step [175/1916], Loss: 0.8319\n",
      "Epoch [2/30], Step [200/1916], Loss: 0.8912\n",
      "Epoch [2/30], Step [225/1916], Loss: 0.8635\n",
      "Epoch [2/30], Step [250/1916], Loss: 0.8957\n",
      "Epoch [2/30], Step [275/1916], Loss: 0.9059\n",
      "Epoch [2/30], Step [300/1916], Loss: 0.8863\n",
      "Epoch [2/30], Step [325/1916], Loss: 0.9270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 345\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 345\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 295\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 295\u001b[0m losses \u001b[38;5;241m=\u001b[39m train_model(model, dataloader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m    298\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_to_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39mchar_to_idx,\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx_to_char\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39midx_to_char,\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39mseq_length,\n\u001b[0;32m    303\u001b[0m }, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/language_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 161\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    158\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    162\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    126\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# LSTM forward\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (h0, c0))\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Apply dropout\u001b[39;00m\n\u001b[0;32m    132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1126\u001b[0m         hx,\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1134\u001b[0m     )\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tkinter import Tk, filedialog\n",
    "import time\n",
    "\n",
    "# Try to import PyPDF2, install if missing\n",
    "try:\n",
    "    import PyPDF2\n",
    "except ImportError:\n",
    "    print(\"Installing PyPDF2...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "    import PyPDF2\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def select_pdf_file():\n",
    "    \"\"\"Open a file dialog to select a PDF file.\"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    root.attributes('-topmost', True)  # Bring dialog to front\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select PDF file\",\n",
    "        filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    "    )\n",
    "    root.destroy()\n",
    "    return file_path if file_path else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    print(f\"Extracting text from: {pdf_path}\")\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            print(f\"PDF has {num_pages} pages\")\n",
    "            \n",
    "            for page_num in range(num_pages):\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"Processing page {page_num+1}/{num_pages}...\")\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            print(f\"Extracted {len(text)} characters\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    print(\"Preprocessing text...\")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s.,?!-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    print(f\"Preprocessed text length: {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=50):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Preprocess text\n",
    "        self.text = preprocess_text(text)\n",
    "        \n",
    "        # Create character vocabulary\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size} characters\")\n",
    "        \n",
    "        # Create training samples\n",
    "        self.samples = []\n",
    "        for i in range(0, len(self.text) - seq_length - 1, 1):\n",
    "            input_seq = [self.char_to_idx[ch] for ch in self.text[i:i+seq_length]]\n",
    "            target_seq = [self.char_to_idx[ch] for ch in self.text[i+1:i+seq_length+1]]\n",
    "            self.samples.append((torch.tensor(input_seq), torch.tensor(target_seq)))\n",
    "        \n",
    "        print(f\"Created {len(self.samples)} training samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2, dropout=0.3):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Reshape for linear layer\n",
    "        out = out.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        # Decode\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=30, learning_rate=0.001):\n",
    "    \"\"\"Train the language model.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device).view(-1)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i+1) % 25 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {time_elapsed:.2f}s')\n",
    "        \n",
    "        # Generate sample after every 5 epochs\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            sample = generate_text(model, train_loader.dataset.char_to_idx, \n",
    "                                   train_loader.dataset.idx_to_char, \"the\", 100, 0.8, \n",
    "                                   train_loader.dataset.seq_length)\n",
    "            print(f\"Sample: {sample}\\n\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_text(model, char_to_idx, idx_to_char, seed_text, length=200, temperature=1.0, seq_length=50):\n",
    "    \"\"\"Generate text using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Process seed text\n",
    "    seed_text = seed_text.lower()\n",
    "    chars = [ch for ch in seed_text if ch in char_to_idx]\n",
    "    \n",
    "    # Handle empty or invalid seed\n",
    "    if not chars:\n",
    "        chars = [' ']\n",
    "    \n",
    "    # Pad if needed\n",
    "    while len(chars) < seq_length:\n",
    "        chars.insert(0, ' ')\n",
    "    \n",
    "    # Keep only the last seq_length characters\n",
    "    chars = chars[-seq_length:]\n",
    "    \n",
    "    # Convert to indices\n",
    "    with torch.no_grad():\n",
    "        indices = [char_to_idx[ch] for ch in chars]\n",
    "        current_seq = torch.tensor([indices]).to(device)\n",
    "        \n",
    "        result = seed_text\n",
    "        \n",
    "        # Generate new characters\n",
    "        for _ in range(length):\n",
    "            # Forward pass\n",
    "            outputs = model(current_seq)\n",
    "            \n",
    "            # Get prediction for the last character\n",
    "            next_char_logits = outputs[-seq_length:].view(-1, len(idx_to_char))[-1]\n",
    "            \n",
    "            # Apply temperature\n",
    "            scaled_logits = next_char_logits / temperature\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probabilities = torch.softmax(scaled_logits, dim=0).cpu().numpy()\n",
    "            \n",
    "            # Sample next character\n",
    "            next_idx = np.random.choice(len(idx_to_char), p=probabilities)\n",
    "            next_char = idx_to_char[next_idx]\n",
    "            \n",
    "            # Add to result\n",
    "            result += next_char\n",
    "            \n",
    "            # Update current sequence\n",
    "            indices = indices[1:] + [next_idx]\n",
    "            current_seq = torch.tensor([indices]).to(device)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Select PDF file using dialog\n",
    "    print(\"Please select a PDF file...\")\n",
    "    pdf_path = select_pdf_file()\n",
    "    \n",
    "    if not pdf_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if not text:\n",
    "        print(\"Failed to extract text from PDF. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Hyperparameters\n",
    "    seq_length = 80\n",
    "    batch_size = 64\n",
    "    embedding_size = 128\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = CharDataset(text, seq_length=seq_length)\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"Dataset is empty. Please provide a PDF with more text.\")\n",
    "        return\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Initializing model...\")\n",
    "    model = SimpleRNN(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        embed_size=embedding_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training on {device}...\")\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    losses = train_model(model, dataloader, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'char_to_idx': dataset.char_to_idx,\n",
    "        'idx_to_char': dataset.idx_to_char,\n",
    "        'seq_length': dataset.seq_length,\n",
    "    }, \"checkpoints/language_model.pth\")\n",
    "    print(\"Model saved to checkpoints/language_model.pth\")\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate text based on user input topic\n",
    "    while True:\n",
    "        topic = input(\"\\nEnter a topic for text generation (or 'quit' to exit): \")\n",
    "        \n",
    "        if topic.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        temp_str = input(\"Enter temperature (0.5-1.5) [default=0.8]: \")\n",
    "        length_str = input(\"Enter length of text to generate [default=300]: \")\n",
    "        \n",
    "        temperature = float(temp_str) if temp_str else 0.8\n",
    "        length = int(length_str) if length_str else 300\n",
    "        \n",
    "        print(f\"\\nGenerating text on '{topic}' (temp={temperature}, length={length})...\\n\")\n",
    "        generated = generate_text(\n",
    "            model,\n",
    "            dataset.char_to_idx,\n",
    "            dataset.idx_to_char,\n",
    "            topic,\n",
    "            length=length,\n",
    "            temperature=temperature,\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(generated)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e412968-adad-44ab-a77f-d29d85676b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d07f5-d511-4ee4-ac56-6497fb093961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e2c31-1905-4b4a-8538-27671ca467d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48304f8f-eaae-4e83-90da-08465cca232c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
